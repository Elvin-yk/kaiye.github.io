<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Kai Ye</title>
  <link rel="stylesheet" href="assets/css/styles.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
  <header class="site-header">
    <div class="container header-inner">
      <h1 class="site-title">&nbsp;</h1>
      <nav class="site-nav">
        <a href="#about">About</a>
        <a href="#education">Education</a>
        <a href="#publications">Publications</a>
        <a href="#work">Work</a>
        <a href="#projects">Projects</a>
      </nav>
    </div>
  </header>

  <main class="container">
    <section class="intro">
      <div class="intro-grid">
        <div class="intro-photo" aria-label="profile photo">
          <img src="assets/profile.jpg" alt="profile" width="160" height="160"/>
        </div>
        <div class="intro-info">
          <p class="intro-name"><strong>Kai Ye</strong></p>
          <p>M.Phil, Computer & Information Engineering</p>
          <p>The Chinese University of Hong Kong, Shenzhen</p>
        </div>
        <div class="intro-contact">
          <p class="intro-email"><strong>Email:</strong> kaiye1@link.cuhk.edu.cn</p>
          <p class="intro-links">
            <a href="https://scholar.google.com/citations?user=g90pENYAAAAJ" target="_blank" rel="noopener">Scholar</a>
            |
            <a href="https://github.com/elvin-yk" target="_blank" rel="noopener">GitHub</a>
            |
            <a class="btn btn--primary" href="assets/Kye_s_Resume.pdf" target="_blank" rel="noopener">Download CV</a>
          </p>
        </div>
      </div>
    </section>

    <section id="about" class="section-block">
      <h2>About</h2>
      <p>
        I am an M.Phil student at The Chinese University of Hong Kong, Shenzhen. 
        My research focuses on the intersection of embodied intelligence and computer vision, including dexterous manipulation and generative modeling.
        My goal is to enable everyone to have an independent robot to collaborate with, create, and live together.
      </p>
    </section>

    <section id="education" class="section-block">
      <h2>Education</h2>
      <div class="entries">
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">The Chinese University of Hong Kong, Shenzhen</div>
            <div class="entry-dates">Jan 2024 – Present</div>
          </div>
          <div class="entry-role entry-title">M.Phil, Computer & Information Engineering</div>
          <ul class="entry-bullets">
            <li>Supervisor: <a href="https://sse.cuhk.edu.cn/faculty/huangrui" target="_blank" rel="noopener">Prof. Rui Huang</a></li>
            <li>Research Assistant, Shenzhen Institute of Artificial Intelligence and Robotics (Jun 2024 – Present)</li>
          </ul>
        </div>
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">Lanzhou University</div>
            <div class="entry-dates">Sept 2018 – July 2022</div>
          </div>
          <div class="entry-role entry-title">B.S., Computer Science & Technology</div>
          <ul class="entry-bullets">
            <li>Supervisor: <a href="https://xxxy.lzu.edu.cn/shiziduiwu/shiyanduiwu/shiyanshigongchengshi/2020/1016/144308.html" target="_blank" rel="noopener">Prof. Minqiang Yang</a></li>
            <li>Research Assistant, UAIS Lab (Mar 2019 – Sept 2021)</li>
          </ul>
        </div>
      </div>
    </section>

    <section id="publications" class="section-block">
      <h2>Publications</h2>
      <div class="pub">
        <div class="pub-thumb">
          <img src="assets/publications/Gen2Real.jpg" alt="Gen2Real" width="140" height="90"/>
        </div>
        <div class="pub-info">
          <p>
            <strong>Gen2Real: Towards Demo-Free Dexterous Manipulation by Harnessing Generated Video</strong>
          </p>
          <p class="pub-authors"><strong>Kai Ye*</strong>, <span class="coauthor">Yuhang Wu*</span>, <span class="coauthor">Shuyuan Hu</span>, <span class="coauthor">Junliang Li</span>, <span class="coauthor">Meng Liu</span>, <span class="coauthor">Yongquan Chen</span>, <span class="coauthor">Rui Huang</span></p>
          <p class="pub-venue">arXiv:2509.14178. Submitted to ICRA.</p>
          <p>
            <a href="https://arxiv.org/abs/2509.14178" target="_blank" rel="noopener">paper</a>
          </p>
        </div>
      </div>
      <div class="pub">
        <div class="pub-thumb">
          <img src="assets/publications/GraspWhatYouWant.jpg" alt="Grasp What You Want" width="140" height="90"/>
        </div>
        <div class="pub-info">
          <p>
            <strong>Grasp What You Want: Embodied Dexterous Grasping System Driven by Your Voice</strong>
          </p>
          <p class="pub-authors"><span class="coauthor">Junliang Li*</span>, <strong>Kai Ye*</strong>, <span class="coauthor">Haolan Kang</span>, <span class="coauthor">Mingxuan Liang</span>, <span class="coauthor">Yuhang Wu</span>, <span class="coauthor">Zhenhua Liu</span>, <span class="coauthor">Huiping Zhuang</span>, <span class="coauthor">Rui Huang</span>, <span class="coauthor">Yongquan Chen</span></p>
          <p class="pub-venue">arXiv:2412.10694. Major revision at Journal of Field Robotics (JFR).</p>
          <p>
            <a href="https://arxiv.org/abs/2412.10694" target="_blank" rel="noopener">paper</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb">
          <img src="assets/publications/IEGAN.jpg" alt="Independent Encoder" width="140" height="90"/>
        </div>
        <div class="pub-info">
          <p>
            <strong>Independent Encoder for Deep Hierarchical Unsupervised Image-to-Image Translation</strong>
          </p>
          <p class="pub-authors"><strong>Kai Ye</strong>, <span class="coauthor">Yinru Ye</span>, <span class="coauthor">Minqiang Yang</span>, <span class="coauthor">Bin Hu</span></p>
          <p class="pub-venue">arXiv:2107.02494.</p>
          <p>
            <a href="https://arxiv.org/abs/2107.02494" target="_blank" rel="noopener">paper</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>Retinal Vessel Segmentation in Medical Diagnosis using Multi-scale Attention Generative Adversarial Networks</strong>
          </p>
          <p class="pub-authors"><span class="coauthor">Minqiang Yang</span>, <span class="coauthor">Yinru Ye</span>, <strong>Kai Ye</strong>, <span class="coauthor">Wei Zhou</span>, <span class="coauthor">Xiping Hu</span>, <span class="coauthor">Bin Hu</span></p>
          <p class="pub-venue">Journal of Mobile Networks and Applications.</p>
          <p>
            <a href="#">paper</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>Retinal Vessel Segmentation Using Multi-scale Generative Adversarial Network with Class Activation Mapping</strong>
          </p>
          <p class="pub-authors"><span class="coauthor">Minqiang Yang</span>, <span class="coauthor">Yinru Ye</span>, <strong>Kai Ye</strong>, <span class="coauthor">Xiping Hu</span>, <span class="coauthor">Bin Hu</span></p>
          <p class="pub-venue">The 10th EAI International Conference on Wireless Mobile Communication and Healthcare (MobiHealth), 2021.</p>
          <p>
            <a href="#">paper</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>A kind of Eyeglasses Try-On System</strong>
          </p>
          <p class="pub-authors"><span class="coauthor">Bin Hu</span>, <span class="coauthor">Minqiang Yang</span>, <strong>Kai Ye</strong>, <span class="coauthor">Yiqi Huang</span>, <span class="coauthor">Yinru Ye</span>, <span class="coauthor">Haoqiu Yan</span></p>
          <p class="pub-venue">National Invention Patent CN112418138B.</p>
          <p>
            <a href="#">patent</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>Lottery system for the right to use underground parking spaces</strong>
          </p>
          <p class="pub-authors"><strong>Kai Ye</strong> (first student author)</p>
          <p class="pub-venue">Software Copyright.</p>
          <p>
            <a href="#">record</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>CAGU-Net: Category Attention Guidance U-Net for Retinal Blood Vessel Segmentation</strong>
          </p>
          <p class="pub-authors"><span class="coauthor">Kexin Sun</span>, <span class="coauthor">Yuelan Xin</span>, <span class="coauthor">Yunliang Qi</span>, <span class="coauthor">Meng Lou</span>, <strong>Kai Ye</strong>, <span class="coauthor">Yinru Ye</span></p>
          <p class="pub-venue">17th International Conference on Computational Intelligence and Security (CIS), IEEE.</p>
          <p>
            <a href="#">paper</a>
          </p>
        </div>
      </div>


    </section>

    <section id="work" class="section-block">
      <h2>Work Experience</h2>
      <div class="entries">
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">YITU Technology, Shanghai</div>
            <div class="entry-dates">Jul 2022 – Jun 2023</div>
          </div>
          <div class="entry-role">Algorithm Engineer — Autonomous Driving</div>
          <ul class="entry-bullets">
            <li>Perception (Jul–Aug 2022): Radar-based replacement model for single-frame occlusion detection; integration and road test</li>
            <li>Motion Prediction (Sept–Dec 2022): 10% improvement over baseline (lower is better); resolved 44.05% real-world road-test issues</li>
            <li>Mapless Driving (Jan–Jun 2023): Data engineering for road-network structure prediction; studied data diversity/density impacts</li>
          </ul>
        </div>
      </div>
    </section>

    <section id="projects" class="section-block">
      <h2>Projects Experience</h2>
      <div class="entries">
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">Eye tracker host system</div>
            <div class="entry-dates">Dec 2020 – Sept 2021</div>
          </div>
          <div class="entry-role">Lead developer</div>
          <ul class="entry-bullets">
            <li>Desktop system for synchronized eye-movement and facial-expression acquisition</li>
          </ul>
        </div>
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">Lottery system for underground parking</div>
            <div class="entry-dates">Mar 2019 – Oct 2019</div>
          </div>
          <div class="entry-role">Independent developer</div>
          <ul class="entry-bullets">
            <li>On-site public lottery system for parking-space allocation</li>
          </ul>
        </div>
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">MODMA dataset website</div>
            <div class="entry-dates">Nov 2019 – Feb 2020</div>
          </div>
          <div class="entry-role">Lead developer</div>
          <ul class="entry-bullets">
            <li>Website for a multimodal emotion dataset — http://modma.lzu.edu.cn/</li>
          </ul>
        </div>
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">Logistics robot</div>
            <div class="entry-dates">Sept 2019 – Oct 2020</div>
          </div>
          <div class="entry-role">Lead developer</div>
          <ul class="entry-bullets">
            <li>Raspberry Pi-based vehicle and robotic-arm control; university-level second prize</li>
          </ul>
        </div>
      </div>
    </section>
  </main>

  <footer class="site-footer">
    <div class="container">
      <p>© 2025 Kai Ye</p>
    </div>
  </footer>
</body>
</html>


