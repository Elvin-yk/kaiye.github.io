<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Kai Ye</title>
  <link rel="stylesheet" href="assets/css/styles.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
  <header class="site-header">
    <div class="container header-inner">
      <h1 class="site-title">&nbsp;</h1>
      <nav class="site-nav">
        <a href="#about">About</a>
        <a href="#education">Education</a>
        <a href="#publications">Publications</a>
        <a href="#work">Work</a>
        <a href="#projects">Projects</a>
      </nav>
    </div>
  </header>

  <main class="container">
    <section class="intro">
      <div class="intro-grid">
        <div class="intro-photo" aria-label="profile photo">
          <img src="assets/profile.jpg" alt="profile" width="160" height="160"/>
        </div>
        <div class="intro-info">
          <p class="intro-name"><strong>Kai Ye</strong></p>
          <p>M.Phil, Computer & Information Engineering</p>
          <p>The Chinese University of Hong Kong, Shenzhen</p>
        </div>
        <div class="intro-contact">
          <p class="intro-email"><strong>Email:</strong> kaiye1@link.cuhk.edu.cn</p>
          <p class="intro-links">
            <a href="https://scholar.google.com/citations?user=g90pENYAAAAJ" target="_blank" rel="noopener">Scholar</a>
            |
            <a href="https://github.com/elvin-yk" target="_blank" rel="noopener">GitHub</a>
            |
            <a class="btn btn--primary" href="assets/Kye_s_Resume.pdf" target="_blank" rel="noopener">Download CV</a>
          </p>
        </div>
      </div>
    </section>

    <section id="about" class="section-block">
      <h2>About</h2>
      <p>
        I am an M.Phil student at The Chinese University of Hong Kong, Shenzhen. 
        My research focuses on the intersection of embodied intelligence and computer vision, including dexterous manipulation and generation model.
        My goal is to enable everyone to have an independent robot to collaborate with, create, and live together.
      </p>
    </section>

    <section id="education" class="section-block">
      <h2>Education</h2>
      <div class="entries">
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">The Chinese University of Hong Kong, Shenzhen</div>
            <div class="entry-dates">Jan. 2024 – Present</div>
          </div>
          <div class="entry-role entry-title">M.Phil, Computer & Information Engineering</div>
          <ul class="entry-bullets">
            <li>Supervisor: <a href="https://sse.cuhk.edu.cn/faculty/huangrui" target="_blank" rel="noopener">Prof. Rui Huang</a></li>
            <li>Research Assistant, Shenzhen Institute of Artificial Intelligence and Robotics (Jun. 2024 – Present)</li>
          </ul>
        </div>
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">Lanzhou University</div>
            <div class="entry-dates">Sep. 2018 – Jul. 2022</div>
          </div>
          <div class="entry-role entry-title">B.S., Computer Science & Technology</div>
          <ul class="entry-bullets">
            <li>Supervisor: <a href="https://xxxy.lzu.edu.cn/shiziduiwu/shiyanduiwu/shiyanshigongchengshi/2020/1016/144308.html" target="_blank" rel="noopener">Prof. Minqiang Yang</a></li>
            <li>Research Assistant, UAIS Lab (Sep. 2020 – Jul. 2022)</li>
          </ul>
        </div>
      </div>
    </section>

    <section id="publications" class="section-block">
      <h2>Publications</h2>
      <div class="pub">
        <div class="pub-thumb">
          <img src="assets/publications/Gen2Real.jpg" alt="Gen2Real" width="140" height="90"/>
        </div>
        <div class="pub-info">
          <p>
            <strong>Gen2Real: Towards Demo-Free Dexterous Manipulation by Harnessing Generated Video</strong>
          </p>
          <p class="pub-authors"><strong>Kai Ye*</strong>, <span class="coauthor">Yuhang Wu*</span>, <span class="coauthor">Shuyuan Hu</span>, <span class="coauthor">Junliang Li</span>, <span class="coauthor">Meng Liu</span>, <span class="coauthor">Yongquan Chen</span>, <span class="coauthor">Rui Huang</span></p>
          <p class="pub-venue">arXiv:2509.14178. Submitted to ICRA 2026.</p>
          <p>
            <a href="https://arxiv.org/abs/2509.14178" target="_blank" rel="noopener">paper</a>
          </p>
        </div>
      </div>
      <div class="pub">
        <div class="pub-thumb">
          <img src="assets/publications/GraspWhatYouWant.jpg" alt="Grasp What You Want" width="140" height="90"/>
        </div>
        <div class="pub-info">
          <p>
            <strong>Grasp What You Want: Embodied Dexterous Grasping System Driven by Your Voice</strong>
          </p>
          <p class="pub-authors"><span class="coauthor">Junliang Li*</span>, <strong>Kai Ye*</strong>, <span class="coauthor">Haolan Kang</span>, <span class="coauthor">Mingxuan Liang</span>, <span class="coauthor">Yuhang Wu</span>, <span class="coauthor">Zhenhua Liu</span>, <span class="coauthor">Huiping Zhuang</span>, <span class="coauthor">Rui Huang</span>, <span class="coauthor">Yongquan Chen</span></p>
          <p class="pub-venue">arXiv:2412.10694. Major revision at Journal of Field Robotics (JFR).</p>
          <p>
            <a href="https://arxiv.org/abs/2412.10694" target="_blank" rel="noopener">paper</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb">
          <img src="assets/publications/IEGAN.jpg" alt="Independent Encoder" width="140" height="90"/>
        </div>
        <div class="pub-info">
          <p>
            <strong>Independent Encoder for Deep Hierarchical Unsupervised Image-to-Image Translation</strong>
          </p>
          <p class="pub-authors"><strong>Kai Ye</strong>, <span class="coauthor">Yinru Ye</span>, <span class="coauthor">Minqiang Yang</span>, <span class="coauthor">Bin Hu</span></p>
          <p class="pub-venue">arXiv:2107.02494.</p>
          <p>
            <a href="https://arxiv.org/abs/2107.02494" target="_blank" rel="noopener">paper</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb">
          <img src="assets/publications/RetinalVesselSegmentationJounral.jpg" alt="Retinal Vessel Segmentation Journal" width="140" height="90"/>
        </div>
        <div class="pub-info">
          <p>
            <strong>Retinal Vessel Segmentation in Medical Diagnosis using Multi-scale Attention Generative Adversarial Networks</strong>
          </p>
          <p class="pub-authors"><span class="coauthor">Minqiang Yang</span>, <span class="coauthor">Yinru Ye</span>, <strong>Kai Ye</strong>, <span class="coauthor">Wei Zhou</span>, <span class="coauthor">Xiping Hu</span>, <span class="coauthor">Bin Hu</span></p>
          <p class="pub-venue">Journal of Mobile Networks and Applications.</p>
          <p>
            <a href="https://drive.google.com/file/u/0/d/1I3f2vbV98dPhOLt1rQOKGOZqVwAmUGRz/view">paper</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb">
          <img src="assets/publications/RetinalVesselSegmentationConference.jpg" alt="Retinal Vessel Segmentation Conference" width="140" height="90"/>
        </div>
        <div class="pub-info">
          <p>
            <strong>Retinal Vessel Segmentation Using Multi-scale Generative Adversarial Network with Class Activation Mapping</strong>
          </p>
          <p class="pub-authors"><span class="coauthor">Minqiang Yang</span>, <span class="coauthor">Yinru Ye</span>, <strong>Kai Ye</strong>, <span class="coauthor">Xiping Hu</span>, <span class="coauthor">Bin Hu</span></p>
          <p class="pub-venue">The 10th EAI International Conference on Wireless Mobile Communication and Healthcare (MobiHealth), 2021.</p>
          <p>
            <a href="https://drive.google.com/file/u/0/d/1w6Jv5R47EJbC6LyIc1bwFumL2KEP_sUI/view">paper</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb">
          <img src="assets/publications/patent.jpg" alt="Eyeglasses Try-On Patent" width="140" height="90"/>
        </div>
        <div class="pub-info">
          <p>
            <strong>A kind of Eyeglasses Try-On System</strong>
          </p>
          <p class="pub-authors"><span class="coauthor">Bin Hu</span>, <span class="coauthor">Minqiang Yang</span>, <strong>Kai Ye</strong>, <span class="coauthor">Yiqi Huang</span>, <span class="coauthor">Yinru Ye</span>, <span class="coauthor">Haoqiu Yan</span></p>
          <p class="pub-venue">National Invention Patent CN112418138B.</p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb">
          <img src="assets/publications/SoftwareCopyright.png" alt="Software Copyright" width="140" height="90"/>
        </div>
        <div class="pub-info">
          <p>
            <strong>Lottery system for the right to use underground parking spaces</strong>
          </p>
          <p class="pub-authors"><strong>Kai Ye</strong></p>
          <p class="pub-venue">Software Copyright.</p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb">
          <img src="assets/publications/CAGU-Net.jpg" alt="CAGU-Net" width="140" height="90"/>
        </div>
        <div class="pub-info">
          <p>
            <strong>CAGU-Net: Category Attention Guidance U-Net for Retinal Blood Vessel Segmentation</strong>
          </p>
          <p class="pub-authors"><span class="coauthor">Kexin Sun</span>, <span class="coauthor">Yuelan Xin</span>, <span class="coauthor">Yunliang Qi</span>, <span class="coauthor">Meng Lou</span>, <strong>Kai Ye</strong>, <span class="coauthor">Yinru Ye</span></p>
          <p class="pub-venue">17th International Conference on Computational Intelligence and Security (CIS), IEEE.</p>
          <p>
            <a href="https://drive.google.com/file/u/0/d/1LytBCzIziggmyVKT6Ta6e0L9HvGYnP1E/view">paper</a>
          </p>
        </div>
      </div>


    </section>

    <section id="work" class="section-block">
      <h2>Work Experience</h2>
      <div class="entries">
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">YITU Technology, Shanghai</div>
            <div class="entry-dates">Jul. 2022 – Jun. 2023</div>
          </div>
          <div class="entry-role">Algorithm Engineer — Autonomous Driving</div>
          <ul class="entry-bullets">
            <li>Trajectory Prediction: Developed network architecture optimization and loss function design approaches; achieved 10% improvement over baseline and resolved 44.05% real-world road-test issues</li>
            <li>Mapless Driving: Data engineering for road-network structure prediction; studied data diversity/density impacts</li>
          </ul>
        </div>
      </div>
    </section>

    <section id="projects" class="section-block">
      <h2>Projects Experience</h2>
      <div class="pub">
        <div class="pub-thumb">
          <img src="assets/projects/GraspingSystem.jpg" alt="Dexterous Grasping System" width="140" height="90"/>
        </div>
        <div class="pub-info">
          <div class="entry-header">
            <div class="entry-institution"><strong>Voice-Controlled Dexterous Grasping System</strong></div>
            <div class="entry-dates">Sep. 2024 – Dec. 2024</div>
          </div>
          <p>Featured on CCTV during the 2024 China International Hi-Tech Fair</p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb">
          <img src="assets/projects/ClassPlatform.jpg" alt="Online Class Platform" width="140" height="90"/>
        </div>
        <div class="pub-info">
          <div class="entry-header">
            <div class="entry-institution"><strong>Online Class Auto-Directing System</strong></div>
            <div class="entry-dates">Jul. 2024 – Aug. 2024</div>
          </div>
          <p>Automatic camera switching for online classes through behavior analysis of professors and students.</p>
          <p><a href="https://gitee.com/elvinyk/class_platform" target="_blank" rel="noopener">https://gitee.com/elvinyk/class_platform</a></p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb">
          <img src="assets/projects/Competition.png" alt="CVPR 2024 Competition" width="140" height="90"/>
        </div>
        <div class="pub-info">
          <div class="entry-header">
            <div class="entry-institution"><strong>CVPR 2024 Autonomous Driving Competition</strong></div>
            <div class="entry-dates">Mar. 2024 – Jun. 2024</div>
          </div>
          <p>Participated on behalf of the laboratory and achieved 5th place out of 64 teams in the Multi-View 3D Visual Grounding track.</p>
          <p><a href="https://opendrivelab.com/challenge2024" target="_blank" rel="noopener">https://opendrivelab.com/challenge2024</a></p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb">
          <img src="assets/projects/Eyetracker.jpg" alt="Eye tracker system" width="140" height="90"/>
        </div>
        <div class="pub-info">
          <div class="entry-header">
            <div class="entry-institution"><strong>Eye tracker host system</strong></div>
            <div class="entry-dates">Dec. 2020 – Sep. 2021</div>
          </div>
          <p>Desktop system for synchronized eye-movement and facial-expression acquisition</p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb">
          <img src="assets/projects/Modma.png" alt="MODMA Dataset" width="140" height="90"/>
        </div>
        <div class="pub-info">
          <div class="entry-header">
            <div class="entry-institution"><strong>MODMA dataset website</strong></div>
            <div class="entry-dates">Nov. 2019 – Feb. 2020</div>
          </div>
          <p>Website for a multimodal emotion dataset.</p>
          <p><a href="http://modma.lzu.edu.cn/" target="_blank" rel="noopener">http://modma.lzu.edu.cn/</a></p>
        </div>
      </div>
    </section>
  </main>

  <footer class="site-footer">
    <div class="container">
      <p>© 2025 Kai Ye</p>
    </div>
  </footer>
</body>
</html>


