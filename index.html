<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Kai Ye</title>
  <link rel="stylesheet" href="assets/css/styles.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
  <header class="site-header">
    <div class="container header-inner">
      <h1 class="site-title">&nbsp;</h1>
      <nav class="site-nav">
        <a href="#about">About</a>
        <a href="#education">Education</a>
        <a href="#research">Research</a>
        <a href="#publications">Publications</a>
        <a href="#work">Work</a>
        <a href="#projects">Projects</a>
      </nav>
    </div>
  </header>

  <main class="container">
    <section class="intro">
      <div class="intro-grid">
        <div class="intro-photo" aria-label="profile photo">
          <img src="assets/img/profile.jpg" alt="profile" width="160" height="160"/>
        </div>
        <div class="intro-info">
          <p><strong>Kai Ye</strong></p>
          <p>M.Phil, Computer & Information Engineering</p>
          <p>The Chinese University of Hong Kong, Shenzhen</p>
          <p><strong>Email:</strong> kaiye1@link.cuhk.edu.cn</p>
          <p>
            <a href="https://scholar.google.com/citations?user=g90pENYAAAAJ" target="_blank" rel="noopener">Scholar</a>
            |
            <a href="https://github.com/elvin-yk" target="_blank" rel="noopener">GitHub</a>
          </p>
          <p>
            <a class="btn btn--primary" href="assets/Kye_s_Resume.pdf" target="_blank" rel="noopener">Download CV (PDF)</a>
          </p>
        </div>
      </div>
    </section>

    <section id="about" class="section-block">
      <h2>About</h2>
      <p>
        I am an M.Phil student at The Chinese University of Hong Kong, Shenzhen. 
        My research lies at the intersection of robotics, machine learning, and computer vision, 
        including embodied AI, multimodal perception, and image-to-image translation.
      </p>
    </section>

    <section id="education" class="section-block">
      <h2>Education</h2>
      <ul class="list">
        <li>The Chinese University of Hong Kong, Shenzhen — M.Phil, Computer & Information Engineering, Jan. 2024 - Now (Supervisor: Rui Huang)</li>
        <li>Lanzhou University — B.S., Computer Science & Technology, Sept. 2018 - July 2022 (Supervisor: Minqiang Yang)</li>
      </ul>
    </section>

    <section id="research" class="section-block">
      <h2>Research</h2>
      <ul>
        <li>Embodied AI: speech- and vision-driven dexterous grasping</li>
        <li>Multimodal Perception: multi-view 3D visual grounding and understanding</li>
        <li>Image-to-Image Translation: hierarchical modeling and semantic consistency</li>
      </ul>
    </section>

    <section id="publications" class="section-block">
      <h2>Publications</h2>
      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>Grasp What You Want: Embodied Dexterous Grasping System Driven by Your Voice</strong>
          </p>
          <p>Junliang Li*, <strong>Kai Ye*</strong>, Haolan Kang, Mingxuan Liang, Yuhang Wu, Zhenhua Liu, Huiping Zhuang, Rui Huang, Yongquan Chen. arXiv:2412.10694. Under submission to Journal of Field Robotics.</p>
          <p>
            <a href="https://arxiv.org/abs/2412.10694" target="_blank" rel="noopener">paper</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>Independent Encoder for Deep Hierarchical Unsupervised Image-to-Image Translation</strong>
          </p>
          <p><strong>Kai Ye</strong>, Yinru Ye, Minqiang Yang, Bin Hu. arXiv:2107.02494.</p>
          <p>
            <a href="https://arxiv.org/abs/2107.02494" target="_blank" rel="noopener">paper</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>Retinal Vessel Segmentation in Medical Diagnosis using Multi-scale Attention Generative Adversarial Networks</strong>
          </p>
          <p>Minqiang Yang, Yinru Ye, <strong>Kai Ye</strong>, Wei Zhou, Xiping Hu, Bin Hu. Journal of Mobile Networks and Applications.</p>
          <p>
            <a href="#">paper</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>Retinal Vessel Segmentation Using Multi-scale Generative Adversarial Network with Class Activation Mapping</strong>
          </p>
          <p>Minqiang Yang, Yinru Ye, <strong>Kai Ye</strong>, Xiping Hu, Bin Hu. The 10th EAI International Conference on Wireless Mobile Communication and Healthcare (MobiHealth), 2021.</p>
          <p>
            <a href="#">paper</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>A kind of Eyeglasses Try-On System</strong>
          </p>
          <p>First student author. National Invention Patent CN112418138B.</p>
          <p>
            <a href="#">patent</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>Lottery system for the right to use underground parking spaces</strong>
          </p>
          <p>First student author. Software Copyright.</p>
          <p>
            <a href="#">record</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>CAGU-Net: Category Attention Guidance U-Net for Retinal Blood Vessel Segmentation</strong>
          </p>
          <p>Kexin Sun, Yuelan Xin, Yunliang Qi, Meng Lou, <strong>Kai Ye</strong>, Yinru Ye. 17th International Conference on Computational Intelligence and Security (CIS), IEEE.</p>
          <p>
            <a href="#">paper</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>Application of artificial intelligence in new diagnostic and therapeutic pattern of pancreatic diseases and its advances</strong>
          </p>
          <p>Xinlong Chen, <strong>Kai Ye</strong>, WenCe Zhou. Chinese Journal of Medical Physics, 2022.</p>
          <p>
            <a href="#">paper</a>
          </p>
        </div>
      </div>
    </section>

    <section id="work" class="section-block">
      <h2>Work Experience</h2>
      <ul class="list">
        <li>Algorithm Engineer, YITU, Shanghai — Jul 2022 - Jun 2023</li>
        <li>Perception (Jul–Aug 2022): developed a radar-based replacement model for single-frame occlusion detection, integrated and road-tested</li>
        <li>Motion Prediction (Sept–Dec 2022): achieved 10% improvement over baseline (lower is better), resolved 44.05% real-world road-test issues</li>
        <li>Mapless Driving (Jan–Jun 2023): data engineering for road-network structure prediction; explored impacts of data diversity and density</li>
      </ul>
    </section>

    <section id="projects" class="section-block">
      <h2>Projects & Research Experience</h2>
      <ul class="list">
        <li>Shenzhen Institute of Artificial Intelligence and Robotics — RA (Jun 2024 – Now): multi-view 3D visual grounding (CVPR 2024 challenge 5/64); voice+vision dexterous hand grasping system; Excellence Award and media coverage; paper [1]</li>
        <li>UAIS Lab, Lanzhou University — RA (Mar 2019 – Sept 2021): unsupervised I2I (outperformed then SOTA); retinal vessel segmentation (multi-scale attention + GAN + stitching augmentation); eyeglasses try-on platform (patent)</li>
        <li>Eye tracker host system: desktop app for synchronized eye-movement and facial-expression acquisition (used in lab)</li>
        <li>Lottery system: parking-space lottery system for faculty (on-site public draw)</li>
        <li>MODMA dataset site: multimodal emotion dataset website (http://modma.lzu.edu.cn/)</li>
        <li>Logistics robot: Raspberry Pi-based vehicle and arm control; university-level second prize</li>
      </ul>
    </section>
  </main>

  <footer class="site-footer">
    <div class="container">
      <p>© 2025 Kai Ye</p>
    </div>
  </footer>
</body>
</html>


