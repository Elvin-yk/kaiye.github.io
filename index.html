<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Kai Ye</title>
  <link rel="stylesheet" href="assets/css/styles.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
  <header class="site-header">
    <div class="container header-inner">
      <h1 class="site-title">&nbsp;</h1>
      <nav class="site-nav">
        <a href="#about">About</a>
        <a href="#education">Education</a>
        <a href="#publications">Publications</a>
        <a href="#work">Work</a>
        <a href="#projects">Projects</a>
      </nav>
    </div>
  </header>

  <main class="container">
    <section class="intro">
      <div class="intro-grid">
        <div class="intro-photo" aria-label="profile photo">
          <img src="assets/profile.jpg" alt="profile" width="160" height="160"/>
        </div>
        <div class="intro-info">
          <p><strong>Kai Ye</strong></p>
          <p>M.Phil, Computer & Information Engineering</p>
          <p>The Chinese University of Hong Kong, Shenzhen</p>
          <p><strong>Email:</strong> kaiye1@link.cuhk.edu.cn</p>
          <p>
            <a href="https://scholar.google.com/citations?user=g90pENYAAAAJ" target="_blank" rel="noopener">Scholar</a>
            |
            <a href="https://github.com/elvin-yk" target="_blank" rel="noopener">GitHub</a>
          </p>
          <p>
            <a class="btn btn--primary" href="assets/Kye_s_Resume.pdf" target="_blank" rel="noopener">Download CV (PDF)</a>
          </p>
        </div>
      </div>
    </section>

    <section id="about" class="section-block">
      <h2>About</h2>
      <p>
        I am an M.Phil student at The Chinese University of Hong Kong, Shenzhen. 
        My research lies at the intersection of robotics, machine learning, and computer vision, 
        including embodied AI, multimodal perception, and image-to-image translation.
      </p>
    </section>

    <section id="education" class="section-block">
      <h2>Education</h2>
      <div class="entries">
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">The Chinese University of Hong Kong, Shenzhen</div>
            <div class="entry-dates">Jan 2024 – Present</div>
          </div>
          <div class="entry-role entry-title">M.Phil, Computer & Information Engineering</div>
          <ul class="entry-bullets">
            <li>Supervisor: Prof. Rui Huang</li>
          </ul>
        </div>
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">Lanzhou University</div>
            <div class="entry-dates">Sept 2018 – July 2022</div>
          </div>
          <div class="entry-role entry-title">B.S., Computer Science & Technology</div>
          <ul class="entry-bullets">
            <li>Supervisor: Prof. Minqiang Yang</li>
          </ul>
        </div>
      </div>
    </section>

    <section id="publications" class="section-block">
      <h2>Publications</h2>
      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>Gen2Real: Towards Demo-Free Dexterous Manipulation by Harnessing Generated Video</strong>
          </p>
          <p><strong>Kai Ye*</strong>, Yuhang Wu*, Shuyuan Hu, Junliang Li, Meng Liu, Yongquan Chen, Rui Huang. arXiv:2509.14178. Submitted to ICRA.</p>
          <p>
            <a href="https://arxiv.org/abs/2509.14178" target="_blank" rel="noopener">paper</a>
          </p>
        </div>
      </div>
      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>Grasp What You Want: Embodied Dexterous Grasping System Driven by Your Voice</strong>
          </p>
          <p>Junliang Li*, <strong>Kai Ye*</strong>, Haolan Kang, Mingxuan Liang, Yuhang Wu, Zhenhua Liu, Huiping Zhuang, Rui Huang, Yongquan Chen. arXiv:2412.10694. Major revision at Journal of Field Robotics (JFR).</p>
          <p>
            <a href="https://arxiv.org/abs/2412.10694" target="_blank" rel="noopener">paper</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>Independent Encoder for Deep Hierarchical Unsupervised Image-to-Image Translation</strong>
          </p>
          <p><strong>Kai Ye</strong>, Yinru Ye, Minqiang Yang, Bin Hu. arXiv:2107.02494.</p>
          <p>
            <a href="https://arxiv.org/abs/2107.02494" target="_blank" rel="noopener">paper</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>Retinal Vessel Segmentation in Medical Diagnosis using Multi-scale Attention Generative Adversarial Networks</strong>
          </p>
          <p>Minqiang Yang, Yinru Ye, <strong>Kai Ye</strong>, Wei Zhou, Xiping Hu, Bin Hu. Journal of Mobile Networks and Applications.</p>
          <p>
            <a href="#">paper</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>Retinal Vessel Segmentation Using Multi-scale Generative Adversarial Network with Class Activation Mapping</strong>
          </p>
          <p>Minqiang Yang, Yinru Ye, <strong>Kai Ye</strong>, Xiping Hu, Bin Hu. The 10th EAI International Conference on Wireless Mobile Communication and Healthcare (MobiHealth), 2021.</p>
          <p>
            <a href="#">paper</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>A kind of Eyeglasses Try-On System</strong>
          </p>
          <p>First student author. National Invention Patent CN112418138B.</p>
          <p>
            <a href="#">patent</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>Lottery system for the right to use underground parking spaces</strong>
          </p>
          <p>First student author. Software Copyright.</p>
          <p>
            <a href="#">record</a>
          </p>
        </div>
      </div>

      <div class="pub">
        <div class="pub-thumb"></div>
        <div class="pub-info">
          <p>
            <strong>CAGU-Net: Category Attention Guidance U-Net for Retinal Blood Vessel Segmentation</strong>
          </p>
          <p>Kexin Sun, Yuelan Xin, Yunliang Qi, Meng Lou, <strong>Kai Ye</strong>, Yinru Ye. 17th International Conference on Computational Intelligence and Security (CIS), IEEE.</p>
          <p>
            <a href="#">paper</a>
          </p>
        </div>
      </div>


    </section>

    <section id="work" class="section-block">
      <h2>Work Experience</h2>
      <div class="entries">
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">YITU Technology, Shanghai</div>
            <div class="entry-dates">Jul 2022 – Jun 2023</div>
          </div>
          <div class="entry-role">Algorithm Engineer — Autonomous Driving</div>
          <ul class="entry-bullets">
            <li>Perception (Jul–Aug 2022): Radar-based replacement model for single-frame occlusion detection; integration and road test</li>
            <li>Motion Prediction (Sept–Dec 2022): 10% improvement over baseline (lower is better); resolved 44.05% real-world road-test issues</li>
            <li>Mapless Driving (Jan–Jun 2023): Data engineering for road-network structure prediction; studied data diversity/density impacts</li>
          </ul>
        </div>
      </div>
    </section>

    <section id="projects" class="section-block">
      <h2>Projects & Research Experience</h2>
      <div class="entries">
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">Shenzhen Institute of Artificial Intelligence and Robotics</div>
            <div class="entry-dates">Jun 2024 – Present</div>
          </div>
          <div class="entry-role">Research Assistant</div>
          <ul class="entry-bullets">
            <li>Multi-view 3D visual grounding — CVPR 2024 challenge: 5/64</li>
            <li>Dexterous hand grasping system (voice + vision); Excellence Award; media coverage</li>
            <li>Paper: Grasp What You Want [1]</li>
          </ul>
        </div>
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">UAIS Lab, Lanzhou University</div>
            <div class="entry-dates">Mar 2019 – Sept 2021</div>
          </div>
          <div class="entry-role">Research Assistant</div>
          <ul class="entry-bullets">
            <li>Unsupervised image-to-image translation — outperformed then SOTA</li>
            <li>Retinal vessel segmentation — multi-scale attention + GAN + stitching augmentation</li>
            <li>Eyeglasses try-on platform — National Invention Patent</li>
          </ul>
        </div>
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">Eye tracker host system</div>
            <div class="entry-dates">Dec 2020 – Sept 2021</div>
          </div>
          <div class="entry-role">Lead developer</div>
          <ul class="entry-bullets">
            <li>Desktop system for synchronized eye-movement and facial-expression acquisition</li>
          </ul>
        </div>
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">Lottery system for underground parking</div>
            <div class="entry-dates">Mar 2019 – Oct 2019</div>
          </div>
          <div class="entry-role">Independent developer</div>
          <ul class="entry-bullets">
            <li>On-site public lottery system for parking-space allocation</li>
          </ul>
        </div>
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">MODMA dataset website</div>
            <div class="entry-dates">Nov 2019 – Feb 2020</div>
          </div>
          <div class="entry-role">Lead developer</div>
          <ul class="entry-bullets">
            <li>Website for a multimodal emotion dataset — http://modma.lzu.edu.cn/</li>
          </ul>
        </div>
        <div class="entry">
          <div class="entry-header">
            <div class="entry-institution">Logistics robot</div>
            <div class="entry-dates">Sept 2019 – Oct 2020</div>
          </div>
          <div class="entry-role">Lead developer</div>
          <ul class="entry-bullets">
            <li>Raspberry Pi-based vehicle and robotic-arm control; university-level second prize</li>
          </ul>
        </div>
      </div>
    </section>
  </main>

  <footer class="site-footer">
    <div class="container">
      <p>© 2025 Kai Ye</p>
    </div>
  </footer>
</body>
</html>


